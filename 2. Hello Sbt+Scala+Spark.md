## 2. Hello Sbt+Scala+Spark
#### Установить Standalone spark
https://github.com/ribentrop/spark-hello-world/blob/master/spark-standalone-install.md
#### Работа в консоли Spark Shell
Самый простой способ заставить Spark что-то поделать, это запустить Spark Shell.
При этом запустится интерпретатор Scala и запустится сессия Spark, то есть мы будем иметь доступ к спарковскому API. 
```sh
[justribentrop_cloud@sbt-scala-spark-2 ~]$ spark-shell
...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://sbt-scala-spark-2.us-central1-a.c.inspired-muse-262209.internal:4040
Spark context available as 'sc' (master = local[*], app id = local-1578734564827).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/

Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_232)
Type in expressions to have them evaluated.
Type :help for more information.
```
Что здесь интересного:
- Using Spark's default log4j... - не разбирался с логированием в Spark, но, как я понял, по умолчанию Spark пишет в консоль. Для записи в файл надо настраивать конфиг, который подхватится библиотекой log4j. Смотри [spark logging to file](https://github.com/joemccann/dillinger/blob/master/KUBERNETES.md)
- Spark context Web UI available at ... -   Spark откроет "Spark shell application UI" - веб страницу нашего нашего консольного приложения, где можно посмотреть ход выполнения приложения.
- Spark context available as 'sc' ..., - Spark session available as 'spark'. - это основные сущности Spark (объекты) для работы нашего приложения. Вся работа со структурами данных Spark (Spark RDD, Spark Dataframe) будут происходить в контексте этих объектов. Простой код ниже это наглядно проиллюстрирует. Короче говоря, через 'sc' и 'spark' идут обращения к API.  
При написании отдельных Scala программ (OT Dispatcher, например), которые должны выполниться на Spark , но не через Spark shell (а через spark-submit при отправке приложения на кластер) нам надо будет в нашей программе явно указать, что сначала нужно создать Spark сессию и Spark контекст. Но об отдельных программах позже.

#### Spark shell
Для примера можно выполнить в spark-shell последовательно две строчки кода [отсюда](https://dzone.com/articles/wordcount-with-spark-and-scala).
Но лучше эти две строчки превратить в три. В этом случае более нагляден принцип lazy evaluation в Spark, о котором будет ниже.
```sh
val rdd = sc.textFile("your_file")
val counts = rdd.flatMap(_.split(" ")).map(x => (x, 1)).reduceByKey(_ + _)
var res=counts.collect
println(res.toList)
```
Параллельно стоит открыть UI приложеия на 4040 порту и обновлять страницу после ввода каждой из трех строк выше. 
Посмотреть, сколько executors выделяет Spark под задачу, сколько ресурсов отъедает executor.

Как можно заметить, само выполнение (непосредственно расчет) начнется только после команды __collect__. Непосредственный запуск вычислений в Spark происходит только после подачи команды "запускай вычисления, которые я тебе описал  выше" - в случае нашего примера это команда collect. Как я понимаю этот [подход](https://stackoverflow.com/questions/38027877/spark-transformation-why-its-lazy-and-what-is-the-advantage) к вычислением Spark (называется __lazy evaluation__) связан с тем, что в ходе вычислений Spark должен построить граф вычислений  (он же __DAG__ - алгоритм вычислений, если проще) оптимальным образом. А Spark сможет это сделать только понимая весь ход вычислений от начала и до конца. 

Вопросы lazy evaluation, термины DAG, executors, jobs, stages и т.п. для самостоятельного изучения.
Ресурсы:
https://medium.com/@goyalsaurabh66/spark-basics-rdds-stages-tasks-and-dag-8da0f52f0454
https://stackoverflow.com/questions/42263270/what-is-the-concept-of-application-job-stage-and-task-in-spark
https://stackoverflow.com/questions/37528047/how-are-stages-split-into-tasks-in-spark

#### Делаем из нашего scala проекта Spark приложение.
Адаптируем и запустим, созданное в Sbt+Scala приложение для запуска на Spark.
Для этого вокруг нашего единственного исполняемоего "println("Hello")" делаем следующую обертку 
```sh
object WordCount {
  def main(args: Array[String]): Unit = {
    println("Hello")
  }
}
```
Теперь запуситим его на Spark. Для этого используется механизм [spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html). В нашем случае submit будет такой:
```sh
spark-submit --class WordCount --master local /home/justribentrop_cloud/foo-build/target/scala-2.12/hello_2.12-0.1.0-SNAPSHOT.jar
```
#### Переписываем наше Spark приложение так, чтобы было обращение к Spark контексту, структурам данных Spark и т.п.
Пока еще наше приложение по сути не содержит ничего spark-специфического. Изменим это.
```sh
$ vi src/main/scala/example/Hello.scala
```
Перепишем наш основной класс приложения:
```sh
import org.apache.spark.sql.SparkSession

object WordCount {
  def main(args: Array[String]): Unit = {
   
    val spark = SparkSession.builder().master(master = "local").getOrCreate()
    val sc = spark.sparkContext
    
    val rdd = sc.textFile("your_file")
    val counts = rdd.flatMap(_.split(" ")).map(x => (x, 1)).reduceByKey(_ + _).collect()
    println(counts.toList)
    
    sc.stop
  }
}
```
Что здесь интересного:
- _import org.apache.spark.sql.SparkSession_ - библиотека, которая подтянется отсюда: [mvnrepository](https://mvnrepository.com/artifact/org.apache.spark/spark-sql). А уже конкретную версию Spark Sql нужно прописать в sbt (см. ниже)
- Создание главного класса Scala приложения:
```sh
- object WordCount {
  def main(args: Array[String]): Unit = {
```
Пока без объяснений - принимаем  как есть.  Почитать можно тут: [раздел Self-Contained Applications](https://spark.apache.org/docs/latest/quick-start.html) и [раздел Building applications for Spark](https://fizzylogic.nl/2015/11/10/spark-101-writing-your-first-spark-app/)
- создаеются объекты Spark session и Spark context. Вспоминаем, что ранее такие же объекты создавалсь при запуске Spark-shell:
```sh
    val spark = SparkSession.builder().master(master = "local").getOrCreate()
    val sc = spark.sparkContext
```
Про Spark session и Spark context нужно читать отдельно. В зависимости от API (RDD, Dataframe, Dataset), Spark context может не создаваться.
Интересные ссылки: _пока нет_
- Останавливаем работу Spark контекста, иначе ругалки могут быть:
```sh
sc.stop
```
Терперь нужно дополнить built.sbt строкой:
```sh
libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.4"
```
Версия Spark Sql совпадает с версией Spark, на которой мы планируем запускать наш проект:
```sh
$ spark-shell --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.4
      /_/
Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_232
Branch
Compiled by user  on 2019-08-27T21:21:38Z
Revision
Url
Type --help for more information.
```
Теперь выполняем reload в sbt, чтобы загрузить библиотеки из исправленного sbt, далее compile и находим в папке ...target/scala-2.хх собранный jar файл.
Запустим spark submit и получим ошибку типа ClassNotFoundException. Если погуглить, то можно понять, что  внимание при сборке должно уделяться совместимостям сред сборки проекта и исполнения. В нашем сулчае - Spark использует  Scala version 2.11.12 (см. spark-shell --version), а мы в build.sbt прописали 2.12.7. Исправим sbt, выполним reload-compile-package проекта и снова сделаем spark-submit. Обратить внимание на то, что будет создан новый jar в папке с соответствующей (исправленной) версией Scala.

#### Возвращаемся к sbt - "более автоматическая" подготовка проекта под Spark
Выше мы "руками" меняли прокт для запуска в на Spark.
Однако в сообществе существут шаблоны sbt проектов, с уже предконфигуриированными зависимостями и т.п.

```sh
sbt new MrPowers/spark-sbt.g8
```
Автор: https://github.com/MrPowers/spark-daria
Можно заглянуть в build.sbt после создания проекта.
Пока мне больше нечего сказать про "шаблонные" проекты.
